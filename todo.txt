1: Implement K-means on the transpose distribution matrix initialized in the way of agglo.py. This would then work like a k-init implementation, but coming from a different angle (and using the already established k-means algorithm.)
2: Come up with a good score for clusters. Perhaps #connections in-cluster / #connections out-cluster? That gives incentive to add nodes, but not if those nodes aren't well clustered. Could add arbitrary constants (after all, 1 is arbitrary too, right?) to weight how we want - perhaps learn those constants through mechanical turk like something.
3: However, the idea behind this clustering at the end of the day is to use random walks, which is something very unique and powerful to graphs, and unfortunately the #in/#out doesn't really take that into account. One thing we want is, if we start in a cluster, we want to walk around and for the most part stay in that cluster. This of course does give incentive to add nodes, since a one node cluster would likely have you leave it quite fast, but in terms of the cluster node absorption idea I'm using it breaks a bit. Perhaps use a k-length random walk, with a teleportation to cluster probability -- then you wouldn't necessarily be absorbed. Or keep the idea of final absorption, binarize it, perhaps that incents adding nodes because if you have a one node cluster there is cluster prob you get to your own cluster but 1 - cluster prob that you go to some other node and then you'll probably end up in one of their clusters. So cluster prob would likely need to be low.